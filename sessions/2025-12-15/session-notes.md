# 深度学习学习记录 - Session 1
**日期**: 2025-12-15
**时长**: ~2 小时
**主题**: 神经网络基础、前向传播、反向传播

---

## 📋 学生背景评估

### 编程基础
- Python：基本了解，能够编程（形式化方法课程实践经验）
- 深度学习库：未使用过 NumPy、Pandas

### 数学基础
- 研二 CS 专业在读
- 学过：线性代数、微积分、概率论
- **强项**：数学基础扎实 ✅

### 机器学习背景
- 上过机器学习课程，得分 80/100
- 对基本概念有一定理解
- CNN、RNN 等深度学习架构尚未接触

### 学习目标
- 做 AI 相关研究
- 找算法工作
- 个人兴趣驱动

### 学习偏好
- 理论优先，代码作为辅助
- 每天可投入 2 小时学习
- 希望有经典 lab/实验加深理解
- 愿意阅读论文、文章、博客

---

## 🎯 本节学习内容

### 1. 机器学习基础概念澄清

**学生初始理解**：
- 认为机器学习是"简单的分类任务"
- 理解训练过程是通过数据调整参数
- 对验证集和测试集的区别不清楚
- 过拟合概念理解有误（认为是数据太少导致的）
- 欠拟合概念不记得

**讲解内容**：
- 机器学习不只是分类（还有回归、生成、强化学习等）
- **训练集/验证集/测试集的区别**：
  - 训练集：模型直接学习
  - 验证集：人根据表现调整超参数
  - 测试集：最终评估，不能用来调整
- **过拟合 vs 欠拟合**：
  - 过拟合：模型太复杂，训练好但验证差
  - 欠拟合：模型太简单，训练和验证都差

**学生反馈**：理解并能正确回答后续验证问题 ✅

---

### 2. 矩阵乘法在神经网络中的意义

**学生初始水平**：
- 知道 (3×4) × (4×2) = (3×2) ✅
- 不记得在神经网络中的含义
- 链式法则记得：dz/dx = (dz/dy) × (dy/dx) ✅

**讲解内容**：
- 矩阵乘法表示神经网络的线性变换
- X (样本数×特征数) × W (特征数×神经元数) = 输出
- 每一列权重代表一个神经元的连接
- 链式法则是反向传播的数学基础

---

### 3. 感知机和神经网络基础

**讲解内容**：
- 感知机的数学定义：z = w^T x + b, y = sign(z)
- 几何理解：用直线分割空间
- **XOR 问题**：线性不可分的经典案例
- 多层网络的解决方案

**重点示例**：两层网络解决 XOR
- 用具体权重和偏置演示
- 逐个输入验证计算过程
- 强调隐藏层的作用：特征变换

**学生关键理解**：
> "通过引入隐藏层，将原本数据做一次映射，映射成线性可分的状态"

**评价**：✅ 完美抓住了深度学习的核心思想！

---

### 4. 激活函数的必要性

**讲解内容**：
- 为什么需要非线性激活函数
- 多层线性变换 = 单层线性变换
- 常用激活函数：
  - Sigmoid: σ(z) = 1/(1+e^(-z))
  - ReLU: max(0, z)
  - Tanh: (e^z - e^(-z))/(e^z + e^(-z))

**学生理解验证**：
- ReLU(-3) = 0 ✅
- ReLU(5) = 5 ✅

---

### 5. 前向传播和反向传播（核心内容）

#### 前向传播
```
z⁽¹⁾ = W⁽¹⁾x + b⁽¹⁾
a⁽¹⁾ = σ(z⁽¹⁾)
z⁽²⁾ = W⁽²⁾a⁽¹⁾ + b⁽²⁾
a⁽²⁾ = σ(z⁽²⁾)
L = (a⁽²⁾ - y)²
```

#### 反向传播
**关键公式推导**：
```
δ⁽²⁾ = ∂L/∂z⁽²⁾ = 2(a⁽²⁾ - y) · σ'(z⁽²⁾)
δ⁽¹⁾ = (W⁽²⁾)ᵀ · δ⁽²⁾ ⊙ σ'(z⁽¹⁾)

∂L/∂W⁽ˡ⁾ = δ⁽ˡ⁾ · (a⁽ˡ⁻¹⁾)ᵀ
∂L/∂b⁽ˡ⁾ = δ⁽ˡ⁾
```

**学生理解检查**：

问题1: 反向传播为什么叫"反向"？
> 回答：因为链式法则，我们求出来的先是高层的梯度，和前向传播的顺序相反 ✅

问题2: 链式法则在反向传播中起什么作用？
> 回答：奠定了反向传播的底层数学原理，也决定了反向传播中的反向两个字 ✅

问题3: 为什么需要保存前向传播的中间值？
> 回答：要用到啊，最后的梯度计算公式的结果，是和前向传播时的 z⁽ˡ⁾ 和 a⁽ˡ⁾ 有关的 ✅

**评价**：三个问题全部正确，说明对核心概念理解深刻！

---

## 💻 编程实践：从零实现神经网络

### 任务
实现两层神经网络解决 XOR 问题（不使用 PyTorch/TensorFlow）

### 代码位置
`/Users/thisingl/Downloads/AI-study/project/backpropagation/example.py`

### 遇到的问题

**Bug 1: 反向传播矩阵乘法顺序错误**
```python
# 学生的代码（错误）
delta1 = delta2 @ W2.T * sigmoid_derivative(Z1)

# 正确的代码
delta1 = (W2.T @ delta2) * sigmoid_derivative(Z1)
```

**错误原因**：
- 维度不匹配：delta2 (1,4) @ W2.T (4,1) = (1,1) ❌
- 正确维度：W2.T (4,1) @ delta2 (1,4) = (4,4) ✅

**Bug 2: 学习率设置**
- 初始设置：0.1（太小）
- 修改为：1.0

### 修复后的结果

**训练过程**：
```
Iteration 0: Cost = 0.252779
Iteration 1000: Cost = 0.000909
Iteration 9000: Cost = 0.000063
```

**最终预测**：
```
输入        预测         真实
(0,0) → 0.0088  ≈ 0  vs  0  ✅
(0,1) → 0.9929  ≈ 1  vs  1  ✅
(1,0) → 0.9930  ≈ 1  vs  1  ✅
(1,1) → 0.0068  ≈ 0  vs  0  ✅
```

**100% 准确率！** ✅

---

## 🧪 超参数实验

### 实验1: 不同隐藏层大小

学生发现：`hidden_size=2` 也能学会 XOR ✅

**分析**：
- 理论上 2 个神经元足够（一个识别 OR，一个识别 AND）
- 更多神经元 → 更多解的路径 → 更容易训练
- 类比：独木桥 vs 宽阔的路

### 实验2: 不同学习率对比

**学生观察**：学习率 10.0 效果更好

**实验验证**（6 种学习率对比）：
| 学习率 | 最终损失 | 观察 |
|--------|----------|------|
| 0.1    | 0.002973 | 收敛慢 |
| 0.5    | 0.000271 | 较快 |
| 1.0    | 0.000120 | 快 |
| 2.0    | 0.000063 | 很快 |
| 5.0    | 0.000026 | 超快 |
| 10.0   | 0.000013 | **最快最好** |

**深层分析**：
- XOR 问题简单 → 损失函数平滑 → 大学习率有效
- 真实复杂问题 → 损失函数崎岖 → 大学习率会震荡/发散
- 实践中常用：Adam (lr=0.001), SGD (lr=0.01) + 学习率衰减

---

## 📊 知识点掌握情况

### 已掌握 ✅
- [x] 训练集/验证集/测试集的区别（高）
- [x] 过拟合 vs 欠拟合的概念（高）
- [x] 矩阵乘法在神经网络中的含义（中高）
- [x] 感知机的数学定义和局限性（高）
- [x] 多层网络解决非线性问题的原理（高）
- [x] 激活函数的必要性（高）
- [x] 前向传播的完整流程（高）
- [x] 反向传播的数学推导（高）
- [x] 链式法则的应用（高）
- [x] 梯度下降的参数更新（高）
- [x] 从零实现神经网络（高）
- [x] 矩阵维度分析和调试（中高）
- [x] 学习率对训练的影响（高）

### 待加强 ⚠️
- [ ] PyTorch/NumPy 的熟练使用（低）
- [ ] 更复杂网络的实现（未涉及）
- [ ] 梯度消失/爆炸问题（下节课）
- [ ] 现代激活函数（ReLU 及变种，下节课）
- [ ] 权重初始化策略（下节课）

---

## 🎯 学习亮点

1. **数学基础扎实**：链式法则、矩阵乘法理解很好
2. **主动思考**：XOR 例子没看懂时主动提问
3. **实验精神**：完成代码后主动尝试不同超参数
4. **观察敏锐**：发现学习率 10.0 效果更好
5. **理解深刻**：能用自己的话准确总结核心概念

**特别突出**：
> "通过引入隐藏层,将原本数据做一次映射,映射成线性可分的状态"
这句话抓住了深度学习的本质！

---

## 💡 知识盲点识别

### 高优先级
- **NumPy 操作不熟练**：矩阵乘法顺序出错
  - 解决方案：多做维度分析练习
  - 建议：每次写矩阵运算时先写下维度

### 中优先级
- **理论与实践的权衡**：以为大学习率总是不好
  - 解决方案：更多实验，理解问题依赖性
  - 已通过对比实验纠正 ✅

---

## 📝 课后建议

### 作业（可选但推荐）
1. **总结笔记**：反向传播是什么？为什么矩阵乘法顺序重要？
2. **扩展实验**：实现三层网络（输入→隐藏1→隐藏2→输出）
3. **阅读材料**：
   - 3Blue1Brown 神经网络视频系列
   - Stanford CS231n Lecture 4 (Backpropagation)

### 下节课预告（2025-12-16）
1. 梯度消失/爆炸问题
2. ReLU 及其变种（LeakyReLU, ELU, GELU）
3. 权重初始化策略（Xavier, He 初始化）
4. Batch Normalization

---

## 📈 整体评价

**学习态度**: ⭐⭐⭐⭐⭐
- 积极提问，主动实验，思考深入

**理解程度**: ⭐⭐⭐⭐⭐
- 核心概念理解准确，能举一反三

**编程能力**: ⭐⭐⭐⭐
- 能独立实现代码，但对矩阵操作需加强

**进度**: 超预期 ✅
- 第一节课就完成了完整的神经网络实现
- 对反向传播的数学推导理解深刻

**下节课重点**: 继续保持理论+实践结合，重点强化 NumPy/PyTorch 操作
